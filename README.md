# Transcription (speech to text) dâ€™Ã©changes tÃ©lÃ©phoniques adressÃ©s au support IT dâ€™une entreprise

Ce projet donne des ressources pour transcrire des conversations tÃ©lÃ©phoniqe ainsi que des outils pour Ã©valuer leurs performances.

## Organisation
Notre code s'articule autour de trois modules :

* La couche d'entrÃ©e
* La couche de traduction
* La couche de sortie

<a href="url"><img src="https://github.com/cmauget/speech-to-text-benchmark/blob/main/utils/diag.png" height="383" width="325" ></a>

  
  
## Setup
Nous utilisons de nombreuses bibliothÃ¨ques python ainsi que des modÃ¨les tel que :
* [Speechrabin](https://speechbrain.github.io/)
* [Sphinx](https://github.com/bambocher/pocketsphinx-python)
* [Vosk](https://alphacephei.com/vosk/)
* [Whisper](https://github.com/openai/whisper)  
  
Pour faciliter l'installation nous avons crÃ©Ã© un environnement [conda](https://docs.conda.io/en/latest/) avec toute les dÃ©pendances. Vous pouvez trouver le fichier de configuration dans utils/environment.yaml.  

---------------------------------------
Vous pouvez changer le nom de l'environnement (wikit par dÃ©faut) en changeant le nom Ã©crit Ã  la premiÃ¨re ligne. Il faudra par la suite modifiÃ© le wikit par le nom choisit.

Vous devez au sein de environment.yaml changer la derniÃ¨re ligne :

    prefix: !modify !this ~/anaconda3/envs/wikit

Par le chemin d'installation de conda, ex :

    prefix: /home/user/anaconda3/envs/wikit
    
Il ne reste plus qu'a installer l'environnement avec la commande : 

    conda env create -f environment.yml  
    
 
---------------------------------------

    
Il faudra de plus tÃ©lÃ©charger les modÃ¨les necessaire pour faire tourner le code. Ils sont disponible dans ce [Drive](https://drive.google.com/drive/folders/1J2lzr-wJGA_9SSn_876XJvbMTBzvRhkX?usp=share_link)  

Il faudra alors unzip le fichier telecharger et le placer dans le repositories comme ci-dessous:
```
ğŸ“‚ speech-to-text-benchmark/ # this is root
â”œâ”€â”€ ğŸ“‚ 1-input/
â”œâ”€â”€ ğŸ“‚ 2-data/
â”œâ”€â”€ ğŸ“‚ 3-outputs/
â”œâ”€â”€ ğŸ“‚ models/
â”œâ”€â”€ ğŸ“‚ utils/
â”‚...
```
Il faut par la suite copier le fichier _utils/fr-FR_ au sein du dosssier :  

    ~/anaconda3/envs/wikit/lib/python3.10/site-packages/speech_recognition/pocketsphinx-data/ 
     
Pour utiliser Whisper il faut Ã©galement installer ffmpeg (sous Ubuntu) :

    sudo apt update && sudo apt install ffmpeg
    
    
*Remarque : bien que nous ayons fait le maximum pour garantir un setup facile, il peut Ãªtre necessaire d'installer d'autres dÃ©pendances. Nous vous invitons a regarder les erreurs d'executuion si jamais*

## Fonctionnement  

Il faut dÃ©poser le fichier .wav Ã  transcrire dans le dossier 1-input/ :  

```
ğŸ“‚ speech-to-text-benchmark/ 
â”œâ”€â”€ ğŸ“‚ 1-input/
|       â”œâ”€â”€ ğŸ“œ audio.wav
|       |...
â”œâ”€â”€ ğŸ“‚ 2-data/
â”‚...
```
Si toutes les dÃ©pendances sont correctements installÃ©es il suffit alors de lancer les commandes suivantes. 

    cd 2-code  
    
puis 

    python3 main.py <nom-du-fichier.wav> <nom-du-fichier-de-sortie.json>

Le fichier de sortie sera alors dans le dossier 3-outpout/

```
ğŸ“‚ speech-to-text-benchmark/ # this is root
|...
â”œâ”€â”€ ğŸ“‚ 3-output/
|       â”œâ”€â”€ ğŸ“œ sortie.json
|       |...
â”‚...
```
